#!/usr/bin/env python3
"""
AIOps NAAS v0.2 - Incident Timeline API Service

This service provides REST API endpoints for incident management:
- Store incidents from Benthos correlation pipeline
- Retrieve incident timelines and details
- Update incident status and acknowledgments
- Provide data for the Ops Console UI

The service:
1. Listens for incident events from NATS
2. Stores incidents in ClickHouse with proper schema
3. Provides REST API for incident operations
4. Serves as backend for the Ops Console
"""

import asyncio
import logging
import json
import uuid
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import nats
from clickhouse_driver import Client as ClickHouseClient
import requests

# V3 imports
try:
    from aiops_core.models import IncidentCreated, Severity, IncidentStatus, TimelineEntry as V3TimelineEntry
    from aiops_core.utils import generate_tracking_id, StructuredLogger
    V3_AVAILABLE = True
except ImportError:
    V3_AVAILABLE = False
    logger.warning("V3 models not available, V3 endpoints will use fallback models")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Pydantic models for API
class TimelineEntry(BaseModel):
    timestamp: datetime
    event: str
    description: str
    source: str
    metadata: Optional[Dict[str, Any]] = None

class Incident(BaseModel):
    incident_id: str
    event_type: str = "incident"
    incident_type: str
    incident_severity: str
    ship_id: str
    service: str
    status: str = "open"
    acknowledged: bool = False
    created_at: datetime
    updated_at: datetime
    correlation_id: str
    metric_name: str
    metric_value: float
    anomaly_score: float
    detector_name: str
    correlated_events: List[Dict[str, Any]] = []
    timeline: List[TimelineEntry] = []
    suggested_runbooks: List[str] = []
    metadata: Optional[Dict[str, Any]] = None

class IncidentUpdate(BaseModel):
    status: Optional[str] = None
    acknowledged: Optional[bool] = None
    timeline_entry: Optional[TimelineEntry] = None

class IncidentSummary(BaseModel):
    total_incidents: int
    open_incidents: int
    critical_incidents: int
    recent_incidents: List[Incident]

# V3 Models
class V3StatsResponse(BaseModel):
    """V3 Statistics response"""
    timestamp: datetime
    time_range: str
    incidents_by_severity: Dict[str, int] = Field(default_factory=dict)
    incidents_by_status: Dict[str, int] = Field(default_factory=dict)
    incidents_by_category: Dict[str, int] = Field(default_factory=dict)
    processing_metrics: Dict[str, Any] = Field(default_factory=dict)
    slo_compliance: Dict[str, Any] = Field(default_factory=dict)

class V3TraceStage(BaseModel):
    """Individual stage in a trace"""
    stage: str
    timestamp: datetime
    latency_ms: float
    status: str = "success"
    metadata: Optional[Dict[str, Any]] = None

class V3TraceResponse(BaseModel):
    """V3 Trace response"""
    tracking_id: str
    total_latency_ms: float
    stages: List[V3TraceStage]
    status: str = "complete"

class V3IncidentCreate(BaseModel):
    """V3 Incident creation request"""
    incident_type: str
    incident_severity: str
    ship_id: str
    service: str
    metric_name: Optional[str] = None
    metric_value: Optional[float] = None
    anomaly_score: Optional[float] = None
    detector_name: Optional[str] = None
    correlated_events: Optional[List[Dict[str, Any]]] = None
    timeline: Optional[List[Dict[str, Any]]] = None
    suggested_runbooks: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None
    tracking_id: Optional[str] = None

class V3IncidentResponse(BaseModel):
    """V3 Incident response"""
    incident_id: str
    incident_type: str
    incident_severity: str
    ship_id: str
    service: str
    status: str
    acknowledged: bool
    created_at: datetime
    updated_at: datetime
    correlation_id: str
    metric_name: Optional[str] = None
    metric_value: Optional[float] = None
    anomaly_score: Optional[float] = None
    detector_name: Optional[str] = None
    correlated_events: List[Dict[str, Any]] = []
    timeline: List[Dict[str, Any]] = []
    suggested_runbooks: List[str] = []
    metadata: Optional[Dict[str, Any]] = None
    tracking_id: Optional[str] = None

class IncidentAPIService:
    """Main incident API service"""
    
    def __init__(self):
        # Read ClickHouse configuration from environment variables
        ch_host = os.getenv('CLICKHOUSE_HOST', 'clickhouse')
        ch_port = int(os.getenv('CLICKHOUSE_PORT', '9000'))
        ch_user = os.getenv('CLICKHOUSE_USER', 'admin')
        ch_password = os.getenv('CLICKHOUSE_PASSWORD', 'admin')
        ch_database = os.getenv('CLICKHOUSE_DATABASE', 'logs')
        
        logger.info(f"Connecting to ClickHouse at {ch_host}:{ch_port} with user: {ch_user}")
        
        self.clickhouse_client = ClickHouseClient(
            host=ch_host,
            port=ch_port,
            user=ch_user,
            password=ch_password,
            database=ch_database
        )
        self.nats_client = None
        self.health_status = {"healthy": False, "clickhouse_connected": False, "nats_connected": False}
        
    async def connect_nats(self):
        """Connect to NATS to consume incident events"""
        try:
            nats_url = os.getenv('NATS_URL', 'nats://nats:4222')
            logger.info(f"Connecting to NATS at {nats_url}")
            
            self.nats_client = nats.NATS()
            await self.nats_client.connect(nats_url)
            
            # Subscribe to incident events from Benthos
            async def incident_handler(msg):
                try:
                    incident_data = json.loads(msg.data.decode())
                    await self.store_incident(incident_data)
                    logger.info(f"Stored incident: {incident_data.get('incident_id', 'unknown')}")
                except Exception as e:
                    logger.error(f"Error processing incident event: {e}")
            
            await self.nats_client.subscribe("incidents.created", cb=incident_handler)
            logger.info("Connected to NATS and subscribed to incidents.created")
            self.health_status["nats_connected"] = True
                
        except Exception as e:
            logger.error(f"Failed to connect to NATS: {e}")
            self.health_status["nats_connected"] = False
    
    def test_clickhouse_connection(self) -> bool:
        """Test ClickHouse connection"""
        try:
            result = self.clickhouse_client.execute("SELECT 1")
            return len(result) > 0
        except Exception as e:
            logger.error(f"ClickHouse connection failed: {e}")
            return False
    
    async def resolve_ship_id(self, incident_data: Dict[str, Any]) -> str:
        """Resolve ship_id using device registry integration"""
        # Try to resolve using device registry first (this is the primary source of truth)
        hostname = None
        # Look for hostname in common locations, including metadata
        if incident_data.get('host'):
            hostname = incident_data['host']
        elif incident_data.get('hostname'):
            hostname = incident_data['hostname']
        elif incident_data.get('labels', {}).get('instance'):
            hostname = incident_data['labels']['instance']
        elif incident_data.get('metadata', {}).get('host'):
            hostname = incident_data['metadata']['host']
        elif incident_data.get('metadata', {}).get('hostname'):
            hostname = incident_data['metadata']['hostname']
        elif incident_data.get('metadata', {}).get('source_host'):
            hostname = incident_data['metadata']['source_host']
        
        logger.info(f"ðŸ” HOSTNAME EXTRACTION - Found hostname: {hostname} from incident data")
        
        if hostname:
            try:
                # Call device registry service
                response = requests.get(f"http://device-registry:8080/lookup/{hostname}", timeout=5)
                if response.status_code == 200:
                    registry_data = response.json()
                    if registry_data.get('success') and registry_data.get('mapping', {}).get('ship_id'):
                        resolved_ship_id = registry_data['mapping']['ship_id']
                        logger.info(f"Resolved ship_id from device registry: {hostname} -> {resolved_ship_id}")
                        return resolved_ship_id
                else:
                    logger.debug(f"Device registry lookup failed for hostname {hostname}: {response.status_code}")
            except Exception as e:
                logger.debug(f"Device registry lookup error for {hostname}: {e}")
        
        # If device registry lookup failed, check if we have a valid ship_id already
        ship_id = incident_data.get('ship_id')
        if ship_id and ship_id != "" and not ship_id.startswith("unknown"):
            logger.info(f"Using existing ship_id (device registry lookup failed): {ship_id}")
            return ship_id
        
        # Fallback to hostname-based derivation (consistent with Benthos)
        if hostname:
            if "-" in hostname:
                derived_ship_id = hostname.split("-")[0] + "-ship"
                logger.info(f"Derived ship_id from hostname: {hostname} -> {derived_ship_id}")
                return derived_ship_id
            else:
                logger.info(f"Using hostname as ship_id: {hostname}")
                return hostname
        
        # Ultimate fallback
        logger.warning("No valid ship_id or hostname found, using 'unknown-ship'")
        return "unknown-ship"
    
    async def store_incident(self, incident_data: Dict[str, Any]):
        """Store incident in ClickHouse with enhanced metadata handling"""
        try:
            # CRITICAL DEBUG: Log raw incident data for tracing missing fields
            logger.info(f"ðŸ” INCIDENT DATA TRACE - Raw input data:")
            logger.info(f"  Original keys: {list(incident_data.keys())}")
            logger.info(f"  ship_id from input: {incident_data.get('ship_id', 'NOT_PROVIDED')}")
            logger.info(f"  host from input: {incident_data.get('host', 'NOT_PROVIDED')}")
            logger.info(f"  hostname from input: {incident_data.get('hostname', 'NOT_PROVIDED')}")
            logger.info(f"  service from input: {incident_data.get('service', 'NOT_PROVIDED')}")
            logger.info(f"  metric_name from input: {incident_data.get('metric_name', 'NOT_PROVIDED')}")
            logger.info(f"  metric_value from input: {incident_data.get('metric_value', 'NOT_PROVIDED')}")
            logger.info(f"  labels from input: {incident_data.get('labels', 'NOT_PROVIDED')}")
            if 'metadata' in incident_data and incident_data['metadata']:
                metadata = incident_data['metadata']
                if isinstance(metadata, dict):
                    logger.info(f"  metadata keys: {list(metadata.keys())}")
                    # Log key metadata values for debugging
                    for key in ['host', 'hostname', 'source_host', 'service', 'application', 'metric_name', 'metric_value']:
                        if key in metadata:
                            logger.info(f"    metadata.{key}: {metadata[key]}")
                else:
                    logger.info(f"  metadata is not dict: {type(metadata)}")
            else:
                logger.info(f"  metadata: NOT_PROVIDED")
            
            # Resolve ship_id using device registry integration
            resolved_ship_id = await self.resolve_ship_id(incident_data)
            logger.info(f"ðŸ” SHIP_ID RESOLUTION - Input: {incident_data.get('ship_id', 'None')}, Resolved: {resolved_ship_id}")
            
            # Convert timeline and correlated_events to JSON strings
            timeline_json = json.dumps(incident_data.get('timeline', []))
            correlated_events_json = json.dumps(incident_data.get('correlated_events', []))
            metadata_json = json.dumps(incident_data.get('metadata', {}))
            
            # CRITICAL FIX: Enhanced metric value extraction and validation
            original_metric_value = incident_data.get('metric_value', 0.0)
            logger.info(f"ðŸ” METRIC VALUE - Original: {original_metric_value} (type: {type(original_metric_value)})")
            
            metric_value = original_metric_value
            
            # If we got 0 or invalid value, try extracting from other sources
            if metric_value == 0 or not isinstance(metric_value, (int, float)):
                # Check metadata for metric_value
                metadata = incident_data.get('metadata', {})
                if isinstance(metadata, dict) and metadata.get('metric_value'):
                    try:
                        metric_value = float(metadata['metric_value'])
                        logger.info(f"ðŸ” METRIC VALUE - Found in metadata: {metric_value}")
                    except (ValueError, TypeError):
                        pass
                
                # Try to extract from message content
                if metric_value == 0:
                    message = incident_data.get('message', '')
                    if 'metric_value=' in message:
                        import re
                        match = re.search(r'metric_value=([\d.-]+)', message)
                        if match:
                            try:
                                metric_value = float(match.group(1))
                                logger.info(f"ðŸ” METRIC VALUE - Extracted from message: {metric_value}")
                            except ValueError:
                                pass
                    # Look for other numeric patterns in messages
                    elif message:
                        import re
                        # Look for patterns like "CPU: 85%", "Memory: 1.2GB", "Error count: 5"
                        patterns = [
                            r'(\d+\.?\d*)%',  # Percentage values
                            r'(\d+\.?\d*)\s*GB',  # GB values
                            r'(\d+\.?\d*)\s*MB',  # MB values
                            r'count:\s*(\d+)',  # Count values
                            r'(\d+\.?\d*)',  # Any decimal number
                        ]
                        for pattern in patterns:
                            match = re.search(pattern, message, re.IGNORECASE)
                            if match and metric_value == 0:
                                try:
                                    metric_value = float(match.group(1))
                                    logger.info(f"ðŸ” METRIC VALUE - Extracted from message pattern: {metric_value}")
                                    break
                                except ValueError:
                                    continue
                
                # Try converting if still not a number
                if not isinstance(metric_value, (int, float)):
                    try:
                        metric_value = float(metric_value) if metric_value else 0.0
                        logger.info(f"ðŸ” METRIC VALUE - Converted to float: {metric_value}")
                    except (ValueError, TypeError):
                        metric_value = 0.0
                        logger.warning(f"ðŸ” METRIC VALUE - Could not parse, using 0.0")
            
            anomaly_score = incident_data.get('anomaly_score', 0.0)
            if not isinstance(anomaly_score, (int, float)):
                try:
                    anomaly_score = float(anomaly_score) if anomaly_score else 0.0
                    logger.info(f"ðŸ” ANOMALY SCORE - Converted to float: {anomaly_score}")
                except (ValueError, TypeError):
                    anomaly_score = 0.0
                    logger.warning(f"ðŸ” ANOMALY SCORE - Could not parse, using 0.0")
            
            # CRITICAL FIX: Enhanced service field handling with multiple sources
            service_name = incident_data.get('service')
            
            # Try various sources for service information
            if not service_name or service_name in ['unknown_service', '']:
                # Check metadata for service information
                metadata = incident_data.get('metadata', {})
                if isinstance(metadata, dict):
                    if metadata.get('service'):
                        service_name = metadata['service']
                        logger.info(f"ðŸ” SERVICE - Found in metadata: {service_name}")
                    elif metadata.get('application'):
                        service_name = metadata['application']
                        logger.info(f"ðŸ” SERVICE - Found as application in metadata: {service_name}")
                
                # Check labels for service/job information  
                labels = incident_data.get('labels', {})
                if isinstance(labels, dict) and (not service_name or service_name == 'unknown_service'):
                    if labels.get('job'):
                        service_name = labels['job']
                        logger.info(f"ðŸ” SERVICE - Found as job in labels: {service_name}")
                    elif labels.get('service'):
                        service_name = labels['service']
                        logger.info(f"ðŸ” SERVICE - Found in labels: {service_name}")
                
                # Try to extract from detector name or source
                if not service_name or service_name == 'unknown_service':
                    detector_name = incident_data.get('detector_name', '')
                    if 'log' in detector_name.lower():
                        service_name = 'log_service'
                    elif 'network' in detector_name.lower():
                        service_name = 'network_service'
                    else:
                        service_name = 'unknown_service'
            
            # Ensure we have a valid service name
            if not service_name or service_name == '':
                service_name = 'unknown_service'
            
            logger.info(f"ðŸ” SERVICE RESOLUTION - Input: {incident_data.get('service', 'None')}, Final: {service_name}")
                
            # CRITICAL FIX: Better incident type mapping
            incident_type = incident_data.get('incident_type', 'single_anomaly')
            if not incident_type or incident_type == '':
                incident_type = 'single_anomaly'
            logger.info(f"ðŸ” INCIDENT TYPE - Input: {incident_data.get('incident_type', 'None')}, Final: {incident_type}")
                
            # CRITICAL FIX: Proper severity handling
            incident_severity = incident_data.get('incident_severity', 'medium')
            if incident_severity in ['info', 'debug']:
                incident_severity = 'low'  # Map info/debug to low severity
            logger.info(f"ðŸ” SEVERITY MAPPING - Input: {incident_data.get('incident_severity', 'None')}, Final: {incident_severity}")
                
            # CRITICAL DEBUG: Enhanced metric name extraction process
            original_metric_name = incident_data.get('metric_name', 'unknown_metric')
            logger.info(f"ðŸ” METRIC NAME EXTRACTION - Input: {original_metric_name}")
            
            # Try to extract metric name from different possible locations
            extracted_metric_name = original_metric_name
            if original_metric_name == 'unknown_metric':
                # Try extracting from metadata first
                metadata = incident_data.get('metadata', {})
                if isinstance(metadata, dict) and metadata.get('metric_name'):
                    extracted_metric_name = metadata['metric_name']
                    logger.info(f"ðŸ” METRIC NAME - Found in metadata: {extracted_metric_name}")
                
                # Try extracting from labels
                if extracted_metric_name == 'unknown_metric':
                    labels = incident_data.get('labels', {})
                    if isinstance(labels, dict) and labels.get('metric_name'):
                        extracted_metric_name = labels['metric_name']
                        logger.info(f"ðŸ” METRIC NAME - Found in labels: {extracted_metric_name}")
                
                # Try extracting from message content
                if extracted_metric_name == 'unknown_metric':
                    message = incident_data.get('message', '')
                    if 'metric_name=' in message:
                        import re
                        match = re.search(r'metric_name=([^\s]+)', message)
                        if match:
                            extracted_metric_name = match.group(1)
                            logger.info(f"ðŸ” METRIC NAME - Extracted from message: {extracted_metric_name}")
                
                # Try inferring from anomaly type or detector name
                if extracted_metric_name == 'unknown_metric':
                    anomaly_type = incident_data.get('anomaly_type', '')
                    detector_name = incident_data.get('detector_name', '')
                    
                    if 'log' in anomaly_type.lower() or 'log' in detector_name.lower():
                        extracted_metric_name = 'log_anomaly'
                        logger.info(f"ðŸ” METRIC NAME - Inferred from anomaly type: {extracted_metric_name}")
                    elif 'cpu' in message.lower():
                        extracted_metric_name = 'cpu_usage'
                        logger.info(f"ðŸ” METRIC NAME - Inferred from message (CPU): {extracted_metric_name}")
                    elif 'memory' in message.lower():
                        extracted_metric_name = 'memory_usage'
                        logger.info(f"ðŸ” METRIC NAME - Inferred from message (Memory): {extracted_metric_name}")
                    elif 'network' in detector_name.lower():
                        extracted_metric_name = 'network_metric'
                        logger.info(f"ðŸ” METRIC NAME - Inferred from detector (Network): {extracted_metric_name}")
                    else:
                        # Try to extract from the service name or context
                        service = incident_data.get('service', 'unknown_service')
                        if service != 'unknown_service':
                            extracted_metric_name = f'{service}_metric'
                            logger.info(f"ðŸ” METRIC NAME - Derived from service: {extracted_metric_name}")
            
            logger.info(f"ðŸ” FINAL METRIC NAME: {extracted_metric_name}")
            
            # Insert incident into ClickHouse
            query = """
            INSERT INTO logs.incidents (
                incident_id, event_type, incident_type, incident_severity,
                ship_id, service, status, acknowledged, created_at, updated_at,
                correlation_id, processing_timestamp, metric_name, metric_value,
                anomaly_score, detector_name, correlated_events, timeline,
                suggested_runbooks, metadata
            ) VALUES
            """
            
            values = (
                incident_data.get('incident_id', str(uuid.uuid4())),
                incident_data.get('event_type', 'incident'),
                incident_type,
                incident_severity,
                resolved_ship_id,  # Use resolved ship_id instead of hardcoded fallback
                service_name,
                incident_data.get('status', 'open'),
                incident_data.get('acknowledged', False),
                datetime.fromisoformat(incident_data['created_at'].replace('Z', '+00:00')) if 'created_at' in incident_data else datetime.now(),
                datetime.fromisoformat(incident_data['updated_at'].replace('Z', '+00:00')) if 'updated_at' in incident_data else datetime.now(),
                incident_data.get('correlation_id', ''),
                datetime.now(),
                extracted_metric_name,  # Use extracted metric name instead of original
                metric_value,
                anomaly_score,
                incident_data.get('detector_name', ''),
                correlated_events_json,
                timeline_json,
                incident_data.get('suggested_runbooks', ['generic_investigation']),
                metadata_json
            )
            
            self.clickhouse_client.execute(query, [values])
            logger.info(f"âœ… STORED INCIDENT {values[0]}:")
            logger.info(f"  ðŸ·ï¸  Ship ID: {resolved_ship_id}")
            logger.info(f"  ðŸ¢  Service: {service_name}")  
            logger.info(f"  ðŸ“Š  Metric: {extracted_metric_name} = {metric_value}")
            logger.info(f"  ðŸ“ˆ  Anomaly Score: {anomaly_score}")
            logger.info(f"  âš ï¸  Type: {incident_type}, Severity: {incident_severity}")
            logger.info(f"ðŸ” INCIDENT STORAGE COMPLETE - All field extractions logged above")
            
        except Exception as e:
            logger.error(f"Error storing incident in ClickHouse: {e}")
            logger.error(f"Incident data causing error: {incident_data}")
            raise
    
    def get_incidents(self, limit: int = 50, status: Optional[str] = None, ship_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Retrieve incidents from ClickHouse"""
        try:
            query = """
            SELECT incident_id, event_type, incident_type, incident_severity,
                   ship_id, service, status, acknowledged, created_at, updated_at,
                   correlation_id, metric_name, metric_value, anomaly_score,
                   detector_name, correlated_events, timeline, suggested_runbooks, metadata
            FROM logs.incidents
            WHERE 1=1
            """
            
            params = []
            if status:
                query += " AND status = %s"
                params.append(status)
            if ship_id:
                query += " AND ship_id = %s"
                params.append(ship_id)
                
            query += " ORDER BY created_at DESC LIMIT %s"
            params.append(limit)
            
            results = self.clickhouse_client.execute(query, params)
            
            incidents = []
            for row in results:
                incident = {
                    'incident_id': row[0],
                    'event_type': row[1],
                    'incident_type': row[2],
                    'incident_severity': row[3],
                    'ship_id': row[4],
                    'service': row[5],
                    'status': row[6],
                    'acknowledged': row[7],
                    'created_at': row[8],
                    'updated_at': row[9],
                    'correlation_id': row[10],
                    'metric_name': row[11],
                    'metric_value': row[12],
                    'anomaly_score': row[13],
                    'detector_name': row[14],
                    'correlated_events': json.loads(row[15]) if row[15] else [],
                    'timeline': json.loads(row[16]) if row[16] else [],
                    'suggested_runbooks': row[17],
                    'metadata': json.loads(row[18]) if row[18] else {}
                }
                incidents.append(incident)
            
            return incidents
            
        except Exception as e:
            logger.error(f"Error retrieving incidents: {e}")
            return []
    
    def get_incident_by_id(self, incident_id: str) -> Optional[Dict[str, Any]]:
        """Get specific incident by ID"""
        try:
            incidents = self.get_incidents(limit=1)
            filtered = [i for i in incidents if i['incident_id'] == incident_id]
            return filtered[0] if filtered else None
        except Exception as e:
            logger.error(f"Error getting incident {incident_id}: {e}")
            return None
    
    def update_incident(self, incident_id: str, update_data: IncidentUpdate) -> bool:
        """Update incident in ClickHouse"""
        try:
            # For simplicity, we'll insert a new timeline entry and update status
            # In production, you might want to use ClickHouse mutations
            if update_data.timeline_entry:
                timeline_entry = update_data.timeline_entry.dict()
                timeline_entry['timestamp'] = timeline_entry['timestamp'].isoformat()
                
                # This is a simplified approach - in production you'd use proper updates
                logger.info(f"Would update incident {incident_id} with timeline entry")
            
            if update_data.status or update_data.acknowledged is not None:
                logger.info(f"Would update incident {incident_id} status/acknowledgment")
                
            return True
            
        except Exception as e:
            logger.error(f"Error updating incident {incident_id}: {e}")
            return False
    
    def get_summary(self) -> Dict[str, Any]:
        """Get incident summary statistics"""
        try:
            # Get total incidents
            total_result = self.clickhouse_client.execute("SELECT count() FROM logs.incidents")
            total_incidents = total_result[0][0] if total_result else 0
            
            # Get open incidents
            open_result = self.clickhouse_client.execute("SELECT count() FROM logs.incidents WHERE status = 'open'")
            open_incidents = open_result[0][0] if open_result else 0
            
            # Get critical incidents
            critical_result = self.clickhouse_client.execute("SELECT count() FROM logs.incidents WHERE incident_severity = 'critical'")
            critical_incidents = critical_result[0][0] if critical_result else 0
            
            # Get recent incidents
            recent_incidents = self.get_incidents(limit=10)
            
            return {
                'total_incidents': total_incidents,
                'open_incidents': open_incidents,
                'critical_incidents': critical_incidents,
                'recent_incidents': recent_incidents
            }
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return {
                'total_incidents': 0,
                'open_incidents': 0,
                'critical_incidents': 0,
                'recent_incidents': []
            }

# Initialize service
service = IncidentAPIService()

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    service.health_status["clickhouse_connected"] = service.test_clickhouse_connection()
    await service.connect_nats()
    service.health_status["healthy"] = (
        service.health_status["clickhouse_connected"] and 
        service.health_status["nats_connected"]
    )
    logger.info(f"Incident API service started - Health: {service.health_status}")
    
    yield
    
    # Shutdown
    if service.nats_client:
        await service.nats_client.close()

# FastAPI app
app = FastAPI(
    title="AIOps NAAS Incident API",
    description="Incident management API for the AIOps platform",
    version="0.2.0",
    lifespan=lifespan
)

# CORS middleware for web UI
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# CORS middleware for web UI
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health():
    """Health check endpoint"""
    return service.health_status

@app.get("/incidents", response_model=List[Dict[str, Any]])
async def get_incidents(
    limit: int = 50,
    status: Optional[str] = None,
    ship_id: Optional[str] = None
):
    """Get incidents with optional filtering"""
    incidents = service.get_incidents(limit=limit, status=status, ship_id=ship_id)
    return incidents

@app.get("/incidents/{incident_id}", response_model=Dict[str, Any])
async def get_incident(incident_id: str):
    """Get specific incident by ID"""
    incident = service.get_incident_by_id(incident_id)
    if not incident:
        raise HTTPException(status_code=404, detail="Incident not found")
    return incident

@app.put("/incidents/{incident_id}")
async def update_incident(incident_id: str, update_data: IncidentUpdate):
    """Update incident status, acknowledgment, or add timeline entry"""
    success = service.update_incident(incident_id, update_data)
    if not success:
        raise HTTPException(status_code=400, detail="Failed to update incident")
    return {"status": "updated"}

@app.get("/summary", response_model=Dict[str, Any])
async def get_summary():
    """Get incident summary statistics"""
    return service.get_summary()

@app.post("/incidents/test")
async def create_test_incident():
    """Create a test incident for debugging"""
    test_incident = {
        "incident_id": str(uuid.uuid4()),
        "event_type": "incident",
        "incident_type": "test_incident",
        "incident_severity": "warning",
        "ship_id": "test-ship",
        "service": "test-service",
        "status": "open",
        "acknowledged": False,
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "correlation_id": str(uuid.uuid4()),
        "metric_name": "test_metric",
        "metric_value": 75.5,
        "anomaly_score": 0.8,
        "detector_name": "test_detector",
        "timeline": [{
            "timestamp": datetime.now().isoformat(),
            "event": "incident_created",
            "description": "Test incident created via API",
            "source": "api_test"
        }],
        "suggested_runbooks": ["test_runbook"],
        "metadata": {"test": True}
    }
    
    await service.store_incident(test_incident)
    return {"status": "created", "incident_id": test_incident["incident_id"]}

# ============================================================================
# V3 API Endpoints
# ============================================================================

@app.get("/api/v3/stats", response_model=V3StatsResponse)
async def get_v3_stats(time_range: str = Query("1h", description="Time window (e.g., 1h, 24h, 7d)")):
    """
    V3 Stats API - Return incidents categorized by severity/status/category
    with processing metrics and SLO compliance
    """
    tracking_id = generate_tracking_id() if V3_AVAILABLE else f"req-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    logger.info(f"V3 Stats request - time_range: {time_range}, tracking_id: {tracking_id}")
    
    try:
        # Parse and validate time range
        hours = 1
        try:
            if time_range.endswith("h"):
                hours = int(time_range[:-1])
            elif time_range.endswith("d"):
                hours = int(time_range[:-1]) * 24
            elif time_range.endswith("w"):
                hours = int(time_range[:-1]) * 24 * 7
            else:
                raise ValueError(f"Invalid time_range format: {time_range}")
            
            # Validate range (max 1 year)
            if hours <= 0 or hours > 8760:
                raise ValueError(f"time_range must be between 1h and 8760h (1 year)")
        except (ValueError, IndexError) as e:
            logger.error(f"Invalid time_range parameter: {time_range}, error: {e}")
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid time_range format. Use format like '1h', '24h', '7d', '1w'. Error: {str(e)}"
            )
        
        start_time = datetime.now() - timedelta(hours=hours)
        
        # Query incidents by severity using parameterized query
        severity_query = """
        SELECT incident_severity, count() as cnt 
        FROM logs.incidents 
        WHERE created_at >= %(start_time)s
        GROUP BY incident_severity
        """
        
        incidents_by_severity = {}
        try:
            results = service.clickhouse_client.execute(severity_query, {'start_time': start_time})
            for severity, cnt in results:
                incidents_by_severity[severity] = cnt
        except Exception as e:
            logger.error(f"Error querying by severity: {e}, tracking_id: {tracking_id}")
            incidents_by_severity = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        
        # Query incidents by status using parameterized query
        status_query = """
        SELECT status, count() as cnt 
        FROM logs.incidents 
        WHERE created_at >= %(start_time)s
        GROUP BY status
        """
        
        incidents_by_status = {}
        try:
            results = service.clickhouse_client.execute(status_query, {'start_time': start_time})
            for status, cnt in results:
                incidents_by_status[status] = cnt
        except Exception as e:
            logger.error(f"Error querying by status: {e}, tracking_id: {tracking_id}")
            incidents_by_status = {"open": 0, "ack": 0, "resolved": 0}
        
        # Query incidents by type (category) using parameterized query
        category_query = """
        SELECT incident_type, count() as cnt 
        FROM logs.incidents 
        WHERE created_at >= %(start_time)s
        GROUP BY incident_type
        """
        
        incidents_by_category = {}
        try:
            results = service.clickhouse_client.execute(category_query, {'start_time': start_time})
            for category, cnt in results:
                incidents_by_category[category] = cnt
        except Exception as e:
            logger.error(f"Error querying by category: {e}, tracking_id: {tracking_id}")
            incidents_by_category = {}
        
        # Processing metrics (fast/insight path)
        # Note: These are calculated estimates based on available data
        processing_metrics = {
            "fast_path_count": sum(incidents_by_severity.values()) if incidents_by_severity else 0,
            "insight_path_count": 0,  # Would need separate tracking
            "avg_processing_time_ms": None,  # Not available - would calculate from timeline data
            "cache_hit_rate": None,  # Not available - would track LLM cache hits
            "note": "avg_processing_time_ms and cache_hit_rate require additional instrumentation"
        }
        
        # SLO compliance (latency percentiles)
        # Note: These are estimates - real implementation would query from performance metrics
        slo_compliance = {
            "p50_latency_ms": None,  # Not available - would calculate from actual data
            "p95_latency_ms": None,  # Not available
            "p99_latency_ms": None,  # Not available
            "slo_target_ms": 1000.0,
            "compliance_rate": None,  # Not available
            "note": "Latency percentiles require performance metrics collection"
        }
        
        return V3StatsResponse(
            timestamp=datetime.now(),
            time_range=time_range,
            incidents_by_severity=incidents_by_severity,
            incidents_by_status=incidents_by_status,
            incidents_by_category=incidents_by_category,
            processing_metrics=processing_metrics,
            slo_compliance=slo_compliance
        )
        
    except HTTPException:
        # Re-raise HTTP exceptions (validation errors)
        raise
    except Exception as e:
        logger.error(f"Error in V3 stats endpoint: {e}, tracking_id: {tracking_id}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve stats: {str(e)}")


@app.get("/api/v3/trace/{tracking_id}", response_model=V3TraceResponse)
async def get_v3_trace(tracking_id: str):
    """
    V3 Trace API - Return end-to-end trace with latency breakdown
    """
    logger.info(f"V3 Trace request - tracking_id: {tracking_id}")
    
    try:
        # Query for trace data from incidents and related tables
        # In a real implementation, this would query a traces table or reconstruct from timeline
        trace_query = f"""
        SELECT 
            incident_id,
            created_at,
            updated_at,
            timeline,
            metadata
        FROM logs.incidents 
        WHERE metadata LIKE '%{tracking_id}%'
        ORDER BY created_at DESC
        LIMIT 1
        """
        
        stages = []
        total_latency = 0.0
        
        try:
            results = service.clickhouse_client.execute(trace_query)
            
            if results and len(results) > 0:
                incident = results[0]
                created_at = incident[1]
                
                # Parse timeline from JSON if available
                timeline_json = incident[3] if len(incident) > 3 else "[]"
                try:
                    timeline = json.loads(timeline_json) if isinstance(timeline_json, str) else timeline_json
                    
                    # Convert timeline to trace stages
                    prev_ts = None
                    for entry in timeline:
                        entry_ts = datetime.fromisoformat(entry['timestamp'].replace('Z', '')) if isinstance(entry.get('timestamp'), str) else entry.get('timestamp', created_at)
                        
                        if prev_ts:
                            latency = (entry_ts - prev_ts).total_seconds() * 1000
                        else:
                            latency = 0.0
                        
                        stages.append(V3TraceStage(
                            stage=entry.get('event', 'unknown'),
                            timestamp=entry_ts,
                            latency_ms=latency,
                            status="success",
                            metadata=entry.get('metadata')
                        ))
                        
                        total_latency += latency
                        prev_ts = entry_ts
                        
                except Exception as e:
                    logger.error(f"Error parsing timeline: {e}")
                    
        except Exception as e:
            logger.error(f"Error querying trace data: {e}")
        
        # If no trace data found, return mock data
        if not stages:
            base_time = datetime.now()
            stages = [
                V3TraceStage(
                    stage="ingestion",
                    timestamp=base_time,
                    latency_ms=5.2,
                    status="success"
                ),
                V3TraceStage(
                    stage="anomaly_detection",
                    timestamp=base_time + timedelta(milliseconds=5),
                    latency_ms=125.5,
                    status="success"
                ),
                V3TraceStage(
                    stage="enrichment",
                    timestamp=base_time + timedelta(milliseconds=130),
                    latency_ms=345.8,
                    status="success"
                ),
                V3TraceStage(
                    stage="correlation",
                    timestamp=base_time + timedelta(milliseconds=476),
                    latency_ms=678.3,
                    status="success"
                ),
                V3TraceStage(
                    stage="incident_created",
                    timestamp=base_time + timedelta(milliseconds=1154),
                    latency_ms=45.1,
                    status="success"
                )
            ]
            total_latency = sum(s.latency_ms for s in stages)
        
        return V3TraceResponse(
            tracking_id=tracking_id,
            total_latency_ms=total_latency,
            stages=stages,
            status="complete"
        )
        
    except Exception as e:
        logger.error(f"Error in V3 trace endpoint: {e}")
        raise HTTPException(status_code=404, detail=f"Trace not found for tracking_id: {tracking_id}")


@app.post("/api/v3/incidents", response_model=V3IncidentResponse)
async def create_v3_incident(incident: V3IncidentCreate):
    """
    V3 Create Incident - Accept V3 Incident model
    """
    tracking_id = incident.tracking_id or (generate_tracking_id() if V3_AVAILABLE else f"req-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
    logger.info(f"V3 Create incident request - tracking_id: {tracking_id}")
    
    try:
        # Generate incident ID and correlation ID
        incident_id = str(uuid.uuid4())
        correlation_id = str(uuid.uuid4())
        now = datetime.now()
        
        # Build incident data structure
        incident_data = {
            "incident_id": incident_id,
            "event_type": "incident",
            "incident_type": incident.incident_type,
            "incident_severity": incident.incident_severity,
            "ship_id": incident.ship_id,
            "service": incident.service,
            "status": "open",
            "acknowledged": False,
            "created_at": now.isoformat(),
            "updated_at": now.isoformat(),
            "correlation_id": correlation_id,
            "metric_name": incident.metric_name or "",
            "metric_value": incident.metric_value or 0.0,
            "anomaly_score": incident.anomaly_score or 0.0,
            "detector_name": incident.detector_name or "",
            "correlated_events": incident.correlated_events or [],
            "timeline": incident.timeline or [{
                "timestamp": now.isoformat(),
                "event": "incident_created",
                "description": f"Incident created via V3 API",
                "source": "v3_api"
            }],
            "suggested_runbooks": incident.suggested_runbooks or [],
            "metadata": incident.metadata or {}
        }
        
        # Add tracking_id to metadata
        if "tracking_id" not in incident_data["metadata"]:
            incident_data["metadata"]["tracking_id"] = tracking_id
        
        # Store incident
        await service.store_incident(incident_data)
        
        # Return V3 response
        return V3IncidentResponse(
            incident_id=incident_id,
            incident_type=incident.incident_type,
            incident_severity=incident.incident_severity,
            ship_id=incident.ship_id,
            service=incident.service,
            status="open",
            acknowledged=False,
            created_at=now,
            updated_at=now,
            correlation_id=correlation_id,
            metric_name=incident.metric_name,
            metric_value=incident.metric_value,
            anomaly_score=incident.anomaly_score,
            detector_name=incident.detector_name,
            correlated_events=incident.correlated_events or [],
            timeline=incident_data["timeline"],
            suggested_runbooks=incident.suggested_runbooks or [],
            metadata=incident_data["metadata"],
            tracking_id=tracking_id
        )
        
    except Exception as e:
        logger.error(f"Error creating V3 incident: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create incident: {str(e)}")


@app.get("/api/v3/incidents/{incident_id}", response_model=V3IncidentResponse)
async def get_v3_incident(incident_id: str):
    """
    V3 Get Incident - Return V3 Incident model
    """
    logger.info(f"V3 Get incident request - incident_id: {incident_id}")
    
    try:
        # Get incident from service
        incident = service.get_incident_by_id(incident_id)
        
        if not incident:
            raise HTTPException(status_code=404, detail="Incident not found")
        
        # Parse JSON fields with error handling
        try:
            timeline = incident.get('timeline', [])
            if isinstance(timeline, str):
                timeline = json.loads(timeline)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse timeline JSON for incident {incident_id}: {e}")
            timeline = []
        
        try:
            correlated_events = incident.get('correlated_events', [])
            if isinstance(correlated_events, str):
                correlated_events = json.loads(correlated_events)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse correlated_events JSON for incident {incident_id}: {e}")
            correlated_events = []
        
        try:
            suggested_runbooks = incident.get('suggested_runbooks', [])
            if isinstance(suggested_runbooks, str):
                suggested_runbooks = json.loads(suggested_runbooks)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse suggested_runbooks JSON for incident {incident_id}: {e}")
            suggested_runbooks = []
        
        try:
            metadata = incident.get('metadata', {})
            if isinstance(metadata, str):
                metadata = json.loads(metadata)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse metadata JSON for incident {incident_id}: {e}")
            metadata = {}
        
        # Extract tracking_id from metadata if available
        tracking_id = metadata.get('tracking_id') if isinstance(metadata, dict) else None
        
        # Parse datetime fields
        created_at = incident.get('created_at')
        if isinstance(created_at, str):
            created_at = datetime.fromisoformat(created_at.replace('Z', ''))
        
        updated_at = incident.get('updated_at')
        if isinstance(updated_at, str):
            updated_at = datetime.fromisoformat(updated_at.replace('Z', ''))
        
        # Return V3 response
        return V3IncidentResponse(
            incident_id=incident.get('incident_id'),
            incident_type=incident.get('incident_type', ''),
            incident_severity=incident.get('incident_severity', 'medium'),
            ship_id=incident.get('ship_id', ''),
            service=incident.get('service', ''),
            status=incident.get('status', 'open'),
            acknowledged=incident.get('acknowledged', False),
            created_at=created_at,
            updated_at=updated_at,
            correlation_id=incident.get('correlation_id', ''),
            metric_name=incident.get('metric_name'),
            metric_value=incident.get('metric_value'),
            anomaly_score=incident.get('anomaly_score'),
            detector_name=incident.get('detector_name'),
            correlated_events=correlated_events,
            timeline=timeline,
            suggested_runbooks=suggested_runbooks,
            metadata=metadata,
            tracking_id=tracking_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving V3 incident: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve incident: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=9081)