# Benthos configuration for AIOps NAAS Sequential Pipeline Correlation
# This configuration implements incident formation and correlation ONLY from
# anomaly.detected.enriched.final - the final stage of the sequential pipeline

http:
  enabled: true
  address: "0.0.0.0:4195"
  debug_endpoints: true

# Input: Consume ONLY fully processed anomalies from the sequential pipeline
input:
  nats:
    urls: ["nats://nats:4222"]
    subject: "anomaly.detected.enriched.final"
    queue: "benthos_correlation"

# Pipeline for sequential correlation and incident formation with LLM/Ollama integration
pipeline:
  processors:
    # Input validation for fully enriched anomaly events from sequential pipeline
    - mapping: |
        # Log enriched input for debugging purposes
        root.debug_input = {
          "raw_content": content(),
          "content_type": content().type(),
          "timestamp": now(),
          "metadata": meta(),
          "pipeline_stage": "benthos_correlation_input"
        }
        
        # Events are already structured and enriched from sequential pipeline
        if content().type() == "string" {
          root = content().parse_json()
        } else if content().type() == "object" {
          root = this
        } else {
          # Fallback for unexpected format
          root = {
            "message": content().string(),
            "level": "ERROR", 
            "timestamp": now(),
            "source": "benthos_correlation_fallback",
            "ship_id": "unknown-ship",
            "error": "unexpected_input_format"
          }
        }

    # Enhanced correlation processing for fully enriched anomaly events
    - mapping: |
        # Generate incident ID and correlation metadata
        root.incident_id = "inc-" + uuid_v4()
        root.correlation_id = if this.correlation_id != null && this.correlation_id != "" { 
          this.correlation_id 
        } else { 
          uuid_v4() 
        }
        root.final_processing_timestamp = now()
        root.pipeline_stage = "benthos_correlation_processing"

    # Preserve all enrichment data from previous stages and create incident
    - mapping: |
        # Ensure all enriched fields are preserved from sequential pipeline
        root.ship_id = if this.ship_id != null && this.ship_id != "" { this.ship_id } else { "unknown-ship" }
        root.ship_name = if this.ship_name != null && this.ship_name != "" { this.ship_name } else { 
          if root.ship_id == "unknown-ship" {
            "Unknown Ship"
          } else if root.ship_id.contains("-ship") && root.ship_id.length() > 5 {
            root.ship_id.slice(0, root.ship_id.length() - 5).split("-").map_each(word -> word.uppercase()).join(" ")
          } else {
            root.ship_id.capitalize()
          }
        }
        
        # Preserve enrichment context and maritime context from Level 1 enrichment
        root.enrichment_context = if this.enrichment_context != null { this.enrichment_context } else { {} }
        root.maritime_context = if this.maritime_context != null { this.maritime_context } else { {} }
        root.operational_status = if this.operational_status != null { this.operational_status } else { "normal" }
        root.llm_enrichment = if this.llm_enrichment != null { this.llm_enrichment } else { {} }
        
        # Preserve all tracking information for traceability
        root.tracking_id = this.tracking_id
        root.log_message = this.log_message
        
        # Core incident fields from enriched anomaly
        root.metric_name = if this.metric_name != null { this.metric_name } else { "unknown_metric" }
        root.metric_value = if this.metric_value != null { this.metric_value } else { 0.0 }
        root.anomaly_score = if this.anomaly_score != null { this.anomaly_score } else { 0.5 }
        root.severity = if this.severity != null { this.severity } else { "medium" }
        root.device_id = if this.device_id != null { this.device_id } else { "unknown-device" }
        root.service = if this.service != null { this.service } else { "unknown_service" }
        root.host = if this.host != null { this.host } else { "unknown" }
        
        # Set incident properties based on enriched data
        root.incident_type = "anomaly_based_incident"
        root.incident_source = "sequential_pipeline"
        root.is_enriched = true
        
    # Store enriched incident events in correlation cache for deduplication
    - cache:
        resource: "correlation_cache"  
        operator: "set"
        key: "${! json(\"ship_id\") + \"_incident_\" + json(\"metric_name\") + \"_\" + (timestamp_unix() / 300 | floor).string() }"
        value: "${! content() }"
        ttl: "600s"  # 10 minute deduplication window
        
    # Store by tracking ID for suppression logic
    - cache:
        resource: "suppression_cache"
        operator: "set"
        key: "${! if json(\"tracking_id\") != null { json(\"tracking_id\") } else { json(\"incident_id\") } }"
        value: "${! content() }"
        ttl: "3600s"  # 1 hour suppression window

    # Apply deduplication and suppression logic
    - switch:
      - check: json("anomaly_score") > 0.8 || json("severity") == "critical"
        processors:
          # High priority incidents - always process
          - mapping: |
              root.suppression_status = "not_suppressed"
              root.suppression_reason = "high_priority_incident"
      
      - check: json("operational_status") == "critical_anomaly"
        processors:
          # Critical operational status - always process
          - mapping: |
              root.suppression_status = "not_suppressed"
              root.suppression_reason = "critical_operational_status"
              
      - processors:
          # Apply suppression for lower priority incidents
          - mapping: |
              root.suppression_status = "suppressed"
              root.suppression_reason = "low_priority_duplicate"

    # LLM/Ollama integration for incident correlation and context enhancement
    - mapping: |
        # Placeholder for LLM/Ollama integration - Use LLM for context enhancement
        root.llm_correlation = {
          "llm_processed": false,  # Set to true when actual LLM integration added
          "correlation_confidence": if this.enrichment_context != {} && this.maritime_context != {} { 0.85 } else { 0.6 },
          "suggested_runbooks": if this.enrichment_context.database_related == true {
            ["investigate_database_connectivity", "check_connection_pool"]
          } else if this.enrichment_context.network_anomaly == true {
            ["check_network_interfaces", "review_traffic_patterns"] 
          } else if this.enrichment_context.system_resource_anomaly == true {
            ["check_resource_utilization", "review_process_list"]
          } else {
            ["general_anomaly_investigation"]
          },
          "incident_summary": if this.log_message != null && this.log_message != "" {
            "Log-based incident: " + this.log_message.slice(0, 100)
          } else {
            "Anomaly detected in " + this.metric_name + " with score " + this.anomaly_score.string()
          }
        }
        
        # Create incident timeline
        root.timeline = [
          {
            "timestamp": this.original_timestamp || now(),
            "event": "anomaly_detected",
            "description": if this.log_message != null && this.log_message != "" {
              "Original log message: " + this.log_message
            } else {
              "Anomaly detected in " + this.metric_name
            },
            "source": "anomaly_detection_service",
            "metadata": {
              "tracking_id": this.tracking_id,
              "anomaly_score": this.anomaly_score
            }
          },
          {
            "timestamp": this.enrichment_timestamp || now(),
            "event": "anomaly_enriched", 
            "description": "Anomaly enriched with maritime and operational context",
            "source": "benthos_enrichment",
            "metadata": {
              "enrichment_context": this.enrichment_context
            }
          },
          {
            "timestamp": now(),
            "event": "incident_created",
            "description": "Incident created from enriched anomaly",
            "source": "benthos_correlation",
            "metadata": {
              "incident_id": this.incident_id,
              "correlation_confidence": this.llm_correlation.correlation_confidence
            }
          }
        ]
        
        # Set final incident status
        root.status = if this.suppression_status == "suppressed" { "suppressed" } else { "open" }
        root.priority = if this.anomaly_score > 0.8 { "high" } else if this.severity == "critical" { "critical" } else { "medium" }
        
        # Add final correlation metadata
        root.correlation_metadata = {
          "pipeline_version": "sequential_v1",
          "processing_stages": ["anomaly_detection", "benthos_enrichment", "enhanced_anomaly_detection", "benthos_correlation"],
          "total_processing_time_ms": (timestamp_unix_milli() - (this.original_timestamp.ts_unix() * 1000)),
          "enrichment_applied": true,
          "llm_correlation_applied": false  # Will be true when LLM integration is added
        }

    # Filter out suppressed incidents
    - switch:
      - check: json("suppression_status") == "suppressed"
        processors:
          - mapping: "root = deleted()"

# Output: Send correlated incidents to incident management
output:
  broker:
    pattern: fan_out
    outputs:
      # Send to NATS for incident management
      - nats:
          urls: ["nats://nats:4222"]
          subject: "incidents.created"
          
      # Log incidents for debugging
      - stdout: {}

# Cache resources for correlation and deduplication
cache_resources:
  - label: "correlation_cache"
    memory:
      default_ttl: "600s"
      compaction_interval: "60s"
      
  - label: "suppression_cache"
    memory:
      default_ttl: "3600s"
      compaction_interval: "300s"

# Metrics and monitoring
metrics:
  prometheus: {}
    
logger:
  level: INFO
  format: json
  add_timestamp: true

# Rate limiting to handle high-throughput data
rate_limit_resources:
  - label: "correlation_rate_limit"
    local:
      count: 1000
      interval: "1s"